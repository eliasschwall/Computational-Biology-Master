---
title: "CompBio Homework SCHWALL ELIAS"
author: "Elias Schwall"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

# Task 1 Pizza and Markov chains

## a) Give the transition matrix to this process
```{r}
transition <- matrix(c(0.4,0.3,0.3,
                       0.0,0.1,0.9,
                       0.1,0.4,0.5), nrow = 3, ncol = 3, byrow = TRUE)

names <- c("Aurora Pizza", "Pizza Boy", "Pizza Company")
rownames(transition) <- names
colnames(transition) <- names
print(transition)
```
## b) Calculate the costumer distribution after 1,2,10 and 20 month
```{r}
time_frames <- c(1,2,10,20)

for (i in time_frames){
    customer_distribution <- c(200,300,100)
    for (j in 1:i) {
        customer_distribution <- customer_distribution %*% transition
    }
    cat("Customer distrinbution after", i, "months",customer_distribution,"\n", sep = " ")
}
```
## c) Interpret the result, using the term ’stationary distribution’
After 10 months we can see that the customer distribution is basically not changing anymore (rounded). This is called the stationary distribution. The stationary distribution is the long term distribution of the customer distribution. This implies that following a specific duration, the customer distribution for each pizza store will reach a stable state, and the market shares will remain relatively consistent over time. In principle the stantionary distribution is achieved earlier than after 10 months, however in the observed time frames we see behavior after 10 months, when comparing it to 20.

# Task 2 Asymmetric random walk - an urn model 
## a) Define the procedure draw and replace in the R-script, which updates the balls in the urn. For weighted sampling, use sample with the option prob= .... The vector can be defined with the weights 1 and s (1 where balls=0 and s where balls=1)
```{r}
#Urn-model#
N <- 100 #number of balls
s <- 2 #weight of white balls

balls <- rep(0,N)
balls[1] <- 1 #start with one white ball

########################################
# Define the function draw and replace #
########################################

draw_and_replace <- function(balls, s=2){
  # Random deletion
  balls <- balls[-sample(length(balls),1)]
  # Weighted duplication
  prob_weights <- balls
  prob_weights[prob_weights == 1] <- s
  prob_weights[prob_weights == 0] <- 1
  selected_ball <- sample(balls, 1, prob = prob_weights)
  balls <- c(balls, selected_ball)
  return(balls)
}

### Try out ###
balls <- draw_and_replace(balls, s=2)  
print(balls)
```
## b) Describe the purpose of the function draw trajectory. Choose at least 3 different parameter combinations (varying s, N, run time etc.) and attach the plots to the pdf. Give a short interpretation of the effects of the parameters.

The draw_trajectory function is used to simulate the urn model and plot the trajectory of the number of balls in the urn over time. The function takes the following parameters: repl is the number of simulations to be run, run_time is the number of time steps to be simulated, s is the weight of the white balls, and N is the number of balls in the urn. The function is used to investigate the effect of the parameters on the urn model. The following plots show the effect of varying s, N, and repl on the urn model.
```{r}
draw_trajectory <- function(repl=20, run_time=1e3, s=2, N=100){
  s <- s
  trajectory <- rep(0,run_time)
  plot(0,0,xlim=c(0,run_time),ylim=c(0,N),xlab="time",ylab="number",main=paste0("s= ",s,", N=",N,", repl= ",repl,", run_time=",run_time,sep=""))
  for (it in 1:repl) {
    balls <- rep(0,N)
    balls[1] <- 1
    
    for (tm in 1:run_time) {
      trajectory[tm] <- sum(balls)
      balls <- draw_and_replace(balls, s)
    }
    
    points(1:run_time,trajectory,type="l")
  }
}
```

### Plots for varying s
```{r}
# Changing s 
draw_trajectory(repl=20, run_time=1e3, s=1, N=100)
draw_trajectory(repl=20, run_time=1e3, s=5, N=100)
draw_trajectory(repl=20, run_time=1e3, s=10, N=100)
```
When changing the weight of the white balls, we can see that the number of white balls in the urn increases faster when the weight is higher. This is because the probability of drawing a white ball is higher when the weight is higher. 
\newpage

### Plots for varying repl
```{r}
# Changing repl
draw_trajectory(repl=20, run_time=1e3, s=2, N=100)
draw_trajectory(repl=30, run_time=1e3, s=2, N=100)
draw_trajectory(repl=50, run_time=1e3, s=2, N=100)
```
When changing the number of simulations, we can see that there are more trajectories. This is because the number of simulations is the number of times the urn model is simulated. Therefore, it more likely that some trajectories will approach the maximum number of white balls in the urn by chance.
\newpage

### Plots for varying N
```{r}
# Changing N
draw_trajectory(repl=20, run_time=1e3, s=2, N=100)
draw_trajectory(repl=20, run_time=1e3, s=2, N=500)
draw_trajectory(repl=20, run_time=1e3, s=2, N=1000)
```
When changing the number of balls in the urn, we can see that it is more likely to pick a black ball, because the ratio of white to black balls changes (we still start only with one white ball). Therefore, it is more likely that the trajectory will approach the minimum number of white balls (0) in the urn by chance. 
\newpage

### Plots for varying run_time
```{r}
# Changing run_time
draw_trajectory(repl=20, run_time=1e2, s=2, N=100)
draw_trajectory(repl=20, run_time=1e3, s=2, N=100)
draw_trajectory(repl=20, run_time=1e4, s=2, N=100)
```
When changing the number of time steps, we can see that reaching a minimum or maximum number of white balls in the urn becomes less likely, due to less drawing and replacing steps. 

## c) Describe the purpose of the function get trans prob. Choose at least 10 different parameter combinations (varying x0, s, N). Give a short interpretation of your result.
```{r}
get_trans_prob <- function(x0,N,s,it=1000){
  x_out <- rep(0,it)
  for(tm in 1:it){
    x <- x0
    y <- sample(c(0,1),1,prob=c(1-x/N,x/N))
    if(y==1){
      x <- x-1
    }
    
    z <- sample(c(0,1),1,prob=c(1-(s*x)/(s*x+(N-1)), (s*x)/(s*x+(N-1)) ))
    if(z==1){
      x <- x+1
    }
    x_out[tm] <- x
  }
  return(table(x_out)/it)
}

get_trans_prob(50,100,20)

# Changing x0 
get_trans_prob(40,100,20)
get_trans_prob(60,100,20)
get_trans_prob(80,100,20)

# Changing N
get_trans_prob(50,80,20)
get_trans_prob(50,120,20)
get_trans_prob(50,160,20)

# Changing s
get_trans_prob(50,100,15)
get_trans_prob(50,100,10)
get_trans_prob(50,100,1)
```
The primary objective of the get_trans_prob function is to simulate a random walk process characterized by two sequential steps, both influenced by probabilistic transitions. \newline
When increasing `x0` the probability of assigning `y` to 1 grows, because the probability for getting 1 from the vector `c(0,1)` is calculated by `x/N`. For the second step, the probability of getting a 1 assigned to the variable `z` is calculated by `(s*x)/(s*x+(N-1)` and therefore for having a weight of e.g. 20 the probability of getting 1 grows with increasing `x0`. So the probabilities for both steps are higher for getting 1 with increasing `x0` and therefore the most probable new assignments for `x` are in the first step `x = x - 1` and `x = x + 1` in the second step, which can also be observed in the distribution of the three possible x values in the table as the number of middle values grows with increasing `x0`. \newline
When increasing `N` the probability of assigning the variable `y` to 1 decreases, because of the formula `x/N`. Therefore it is more likely to assign `y` to 0 and `x` not be subtracted by 1. In the second step the probability for assigning `z` to 1 is high due to the strong effect of the weight variable `s` (as discussed above). \newline
When decreasing `s` the probability for assigning the variable `z` to 1 also decreases and therefore it is less likely to add 1 to `x`. This results in a higher frequency of the lowest of the three values. 


## d) Bonus: How can this process be applied to population genetics and evolution?
The described process can be applied to population genetics and evolution by representing the initial allele frequency (`x0`) of a gene in a population (`N`). The parameter (`s`) influences transitions, akin to factors like selection, migration, or mutation rates. The number of iterations (`it`) simulates generations, with each step reflecting genetic events like drift or selection. This process allows to study how evolutionary forces shape allele frequencies over time, offering insights into genetic diversity and population dynamics.


# Task 3 CpG islands and HMMs
## a) Compare the entries of C−G− and C+G+ and interprete the result.
The probability of getting from a C to a G in a non CpG-island is 0.21% while the likelihood of getting from a C to a G in a CpG island is 21.91% (factor 100), which makes sense given the fact that CpG-islands are characterized by a high number CpG-pairs.   

## b) How does the emission matrix look like? What are the observables and the emission
probabilities from the hidden states? (1P)
```{r}
B <- matrix(c(1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1),nrow=8,byrow=T)
rownames(B) <- c("A-","C-","G-","T-","A+","C+","G+","T+")
colnames(B) <- c("A","C","G","T")
B
```
The observables are the A, C, T, G bases without the island status. The emission probabilities from the hidden states has to be 100% for a corresponding base regardless of its island status and therefore the other probabilities in the row have to be 0.  

## c) Use the transition matrix to calculate the probability, that the following sequence was sampled from a CpG-island and the probability, that it was sampled from a non-island region. TGACGACGAA. Compare the probabilities.
```{r}
prob_island <- 0.2910 * 0.2557 * 0.1892 *  0.2191 * 0.2557 * 0.1892 * 0.2191 * 0.2557 * 0.3190
prob_non_island <- 0.2158 * 0.3654 * 0.1629 *  0.0211 * 0.3654 *  0.1629 * 0.0211 * 0.3654 * 0.3921

prob_island > prob_non_island
```
The probability that the sequence is derived 

d) Bonus: Use the R-script on Ilias to determine the most likely sequence of hidden paths, that ’produce’ the sequence which is also available on Ilias. Run the whole script, have a look at the output and answer the following question: What is the benefit of an HMM-approach compared to the standard CG-content measurement? In general: In which scenarios should you rather use the HMM than ’local’ measurements?
